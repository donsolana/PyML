{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Binary Classification \n\n## Introduction\nThe purpose of this notebook is to create a classification model. Not much about the data is known. We are going to follow these steps iteratively:\n\n1. Cleaning\n2. Exploratory Data Analysis\n3. Preprocessing\n4. Feature Engineering\n5. Modelling\n6. Valuation\n","metadata":{}},{"cell_type":"markdown","source":"### Cleaning and Exploration\n\nBefore we model we should first view the data set for inconsistencies and missing values. After observing the rows displayed below, we see that some rows have inconsistent data types and missing values.\n\nWe will follow the following process:\n\n1. Read in data\n2. Add columns to the unnamed columns.\n3. Handle inconsistent data types starting using **Class label** as a benchmark.\n4. Transform Data Types\n5. Handle missing values accordingly.\n6. Find and remove duplicates.\n7. Search for outliers.\n","metadata":{}},{"cell_type":"code","source":"# import statements\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import log_loss, f1_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n\n# Add column names\ncolnames =  ['v1','v2', 'v3', 'v4', 'v33', 'v76', 'v12', 'v68', 'v50', 'v7', 'v70', 'v55', 'v20', 'v24','v32', 'v97', 'v28', 'v99', 'v95', 'v42', 'v53', 'v85', 'v9', 'v84','v44', 'classlabel']\ntrain = pd.read_csv(\"../input/apt-train/Training.csv\", names =  colnames, header = None)\nvalidate = pd.read_csv(\"../input/aptvalidate/Validation.csv\", names =  colnames, header = None)\ntrain = train.iloc[1:len(train.index)]\nvalidate = validate.iloc[1:len(validate.index)]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-01T06:30:21.837943Z","iopub.execute_input":"2022-02-01T06:30:21.838580Z","iopub.status.idle":"2022-02-01T06:30:24.891870Z","shell.execute_reply.started":"2022-02-01T06:30:21.838473Z","shell.execute_reply":"2022-02-01T06:30:24.890768Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:24.894373Z","iopub.execute_input":"2022-02-01T06:30:24.894989Z","iopub.status.idle":"2022-02-01T06:30:24.931185Z","shell.execute_reply.started":"2022-02-01T06:30:24.894931Z","shell.execute_reply":"2022-02-01T06:30:24.929966Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.classlabel.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:24.933086Z","iopub.execute_input":"2022-02-01T06:30:24.933453Z","iopub.status.idle":"2022-02-01T06:30:24.946381Z","shell.execute_reply.started":"2022-02-01T06:30:24.933397Z","shell.execute_reply":"2022-02-01T06:30:24.945268Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Handling inconsistences.\nSome of the columns had inconsistent data types. In many rows values seem to have shifted into neighbouring columns. To handle this we will employ pandas' ***shift*** method. \n\n**Strategy**: We visually identify shifted columns by displaying rows where the **Class Label** is missing. Then we scan for patterns and make rules around such patterns. One of such patterns  is ***shifting rows where v4 = 'f' or 't'***. This pattern comes from the realization that ***v4*** should contain numeric values and these values (***f*** and ***t***) belong to the neighbouring column.\n\nWe identify these problematic columns throughout the dataset and iteratively come up with rules to adjust them. We compose these transformations as a function called **clean**. This allows to repeat our process on the validation set with ease.","metadata":{}},{"cell_type":"code","source":"def clean(dataset):\n    # shifting rows where v4 = 'f' or 't'\n    row_num, col = dataset.shape\n\n    for i in range(row_num):\n        if ((dataset.v4[i+1] == \"f\") | (dataset.v4[i+1] == \"t\")):\n            dataset.iloc[i, 3:] = dataset.iloc[i, 3:].shift(1,fill_value = \"0\")\n\n    # shifting columns where v12 has 'a' or 'b'\n    \n    j = dataset.columns.get_loc(\"v12\")\n    for i in range(row_num):\n        if ((dataset.v12[i+1] == \"a\") | (dataset.v12[i+1] == \"b\")):\n            dataset.iloc[i, 6:] = dataset.iloc[i, 6:].shift(1)\n\n    # shifting columns where v70 does not have 0\n    j = dataset.columns.get_loc(\"v70\")\n    for i in range(row_num):\n        if (dataset.v70[i+1] != \"0\"):\n            dataset.iloc[i, 10:] = dataset.iloc[i, 10:].shift(1)\n\n    # shifting columns where v20 has 'u' or 'y '\n    j = dataset.columns.get_loc(\"v20\")\n    for i in range(row_num):\n        if ((dataset.v20[i+1] == \"u\") | (dataset.v20[i+1] == \"y\")):\n            dataset.iloc[i, j:] = dataset.iloc[i, j:].shift(1)\n        \n    j = dataset.columns.get_loc(\"v24\")\n    for i in range(row_num):\n        if ((dataset.v24[i+1] != \"u\") & (dataset.v24[i+1] != \"y\") & (dataset.v24[i+1] != 'l')):\n            dataset.iloc[i, j:] = dataset.iloc[i, j:].shift(1)\n\n\n    j = dataset.columns.get_loc(\"v97\")\n    for i in range(row_num):\n        if ((dataset.v97[i+1] == \"f\") | (dataset.v97[i+1] == \"t\")):\n            dataset.iloc[i, j:] = dataset.iloc[i, j:].shift(1, fill_value=\"0\")\n\n\n    start = dataset.columns.get_loc(\"v68\")\n    end = dataset.columns.get_loc(\"v70\") + 1\n    for i in range(row_num):\n        if ((dataset.v68[i+1] == 't') | (dataset.v68[i+1] == 'f') ):\n            dataset.iloc[i, start:end] = dataset.iloc[i, start:end].shift(1)\n            \n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:24.949697Z","iopub.execute_input":"2022-02-01T06:30:24.950215Z","iopub.status.idle":"2022-02-01T06:30:24.969190Z","shell.execute_reply.started":"2022-02-01T06:30:24.950174Z","shell.execute_reply":"2022-02-01T06:30:24.968105Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"validate","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:24.971655Z","iopub.execute_input":"2022-02-01T06:30:24.972256Z","iopub.status.idle":"2022-02-01T06:30:25.016273Z","shell.execute_reply.started":"2022-02-01T06:30:24.972202Z","shell.execute_reply":"2022-02-01T06:30:25.015447Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train = clean(train)\nvalidate = clean(validate)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:25.017612Z","iopub.execute_input":"2022-02-01T06:30:25.018697Z","iopub.status.idle":"2022-02-01T06:30:26.595233Z","shell.execute_reply.started":"2022-02-01T06:30:25.018639Z","shell.execute_reply":"2022-02-01T06:30:26.594288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#view results.\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.598487Z","iopub.execute_input":"2022-02-01T06:30:26.598898Z","iopub.status.idle":"2022-02-01T06:30:26.626576Z","shell.execute_reply.started":"2022-02-01T06:30:26.598831Z","shell.execute_reply":"2022-02-01T06:30:26.625686Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Above we see the third and seventh row shifted towards the right.","metadata":{}},{"cell_type":"code","source":"#check for missing or null labels\ntrain.classlabel.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.627949Z","iopub.execute_input":"2022-02-01T06:30:26.628247Z","iopub.status.idle":"2022-02-01T06:30:26.644266Z","shell.execute_reply.started":"2022-02-01T06:30:26.628203Z","shell.execute_reply":"2022-02-01T06:30:26.643081Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"validate.classlabel.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.645760Z","iopub.execute_input":"2022-02-01T06:30:26.646649Z","iopub.status.idle":"2022-02-01T06:30:26.660802Z","shell.execute_reply.started":"2022-02-01T06:30:26.646592Z","shell.execute_reply":"2022-02-01T06:30:26.659566Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.665452Z","iopub.execute_input":"2022-02-01T06:30:26.665722Z","iopub.status.idle":"2022-02-01T06:30:26.695448Z","shell.execute_reply.started":"2022-02-01T06:30:26.665692Z","shell.execute_reply":"2022-02-01T06:30:26.694424Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Let's check if the columns now have more consistent values. By \n1. Printing out unique values in each column","metadata":{}},{"cell_type":"code","source":"validate.apply(set)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.696706Z","iopub.execute_input":"2022-02-01T06:30:26.697321Z","iopub.status.idle":"2022-02-01T06:30:26.719478Z","shell.execute_reply.started":"2022-02-01T06:30:26.697255Z","shell.execute_reply":"2022-02-01T06:30:26.717966Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train.apply(set)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.721114Z","iopub.execute_input":"2022-02-01T06:30:26.721942Z","iopub.status.idle":"2022-02-01T06:30:26.757886Z","shell.execute_reply.started":"2022-02-01T06:30:26.721891Z","shell.execute_reply":"2022-02-01T06:30:26.756895Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Dealing with formating\n\nI pressume that the data was initially skewed on some rows, due to a formatting error. This assume this error was that decimals might have been represented by commas in the original data file.\n\nTo correct this we concatenate those colums together separated by periods**(\".\")**.","metadata":{}},{"cell_type":"code","source":"def reformat(dataset):\n    \"\"\"\n    This function reformats the numerical columns.\n    \n    Arguments: Dataframe\n    \"\"\"\n    ##Converting missing values in the v12 column to zero\n    dataset.v12 = dataset.v12.fillna(\"0\")\n    dataset.v4 = dataset.v4.fillna(\"0\")\n    dataset.v3 = dataset.v3.fillna(\"0\")\n\n    #concatenating columns\n    dataset[\"v5\"] = dataset.v3 + \".\" + dataset.v4\n    dataset[\"v8\"] = dataset.v76 + \".\" + dataset.v12\n    dataset[\"v91\"] = dataset.v70 + \".\"+ dataset.v55\n    dataset[\"v100\"] = dataset.v32 + \".\" + dataset.v97\n\n    # droping previous columns\n    dataset = dataset.drop(columns='v3')\n    dataset = dataset.drop(columns='v4')\n    dataset = dataset.drop(columns='v76')\n    dataset = dataset.drop(columns='v12')\n    dataset = dataset.drop(columns='v70')\n    dataset = dataset.drop(columns='v55')\n    dataset = dataset.drop(columns='v32')\n    dataset = dataset.drop(columns='v97')\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.759326Z","iopub.execute_input":"2022-02-01T06:30:26.759574Z","iopub.status.idle":"2022-02-01T06:30:26.770672Z","shell.execute_reply.started":"2022-02-01T06:30:26.759544Z","shell.execute_reply":"2022-02-01T06:30:26.769525Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train = reformat(train)\nvalidate = reformat(validate)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.772550Z","iopub.execute_input":"2022-02-01T06:30:26.772846Z","iopub.status.idle":"2022-02-01T06:30:26.833101Z","shell.execute_reply.started":"2022-02-01T06:30:26.772801Z","shell.execute_reply":"2022-02-01T06:30:26.832045Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Transform Data Types\nThough we have achieved homogenuity in the dataset, we have all the data set to \"object\" types, so we have to to specify the types.\n\n1. We pick out the numeric columns.\n2. We make the rest category types.\n","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.834659Z","iopub.execute_input":"2022-02-01T06:30:26.834913Z","iopub.status.idle":"2022-02-01T06:30:26.865038Z","shell.execute_reply.started":"2022-02-01T06:30:26.834882Z","shell.execute_reply":"2022-02-01T06:30:26.864390Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Convert columns to integer train\nnumeric_columns = ['v5', \"v8\", 'v7', \"v91\", 'v20','v100', 'v53', 'v42']\ntrain[numeric_columns] = train[numeric_columns].astype('float')\n\n# Convert columns to integer validate\nvalidate[numeric_columns] = validate[numeric_columns].astype('float')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.866136Z","iopub.execute_input":"2022-02-01T06:30:26.866635Z","iopub.status.idle":"2022-02-01T06:30:26.895278Z","shell.execute_reply.started":"2022-02-01T06:30:26.866600Z","shell.execute_reply":"2022-02-01T06:30:26.894242Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Convert object types to categories train\nobject_columns =  train.select_dtypes(include=\"object\").columns.tolist()\ntrain[object_columns] = train[object_columns].astype('category')\n\n#Convert object types to categories validate\nvalidate[object_columns] = validate[object_columns].astype('category')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.896912Z","iopub.execute_input":"2022-02-01T06:30:26.897194Z","iopub.status.idle":"2022-02-01T06:30:26.941543Z","shell.execute_reply.started":"2022-02-01T06:30:26.897162Z","shell.execute_reply":"2022-02-01T06:30:26.940830Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.942967Z","iopub.execute_input":"2022-02-01T06:30:26.943207Z","iopub.status.idle":"2022-02-01T06:30:26.966802Z","shell.execute_reply.started":"2022-02-01T06:30:26.943177Z","shell.execute_reply":"2022-02-01T06:30:26.965598Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Reformating the classlabel column**","metadata":{}},{"cell_type":"code","source":"# Reformating the classlabel column\ntrain.classlabel = np.where(train.classlabel == \"yes.\",1, 0)\nvalidate.classlabel = np.where(validate.classlabel == \"yes.\",1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.968723Z","iopub.execute_input":"2022-02-01T06:30:26.969223Z","iopub.status.idle":"2022-02-01T06:30:26.977427Z","shell.execute_reply.started":"2022-02-01T06:30:26.969174Z","shell.execute_reply":"2022-02-01T06:30:26.976707Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:26.978882Z","iopub.execute_input":"2022-02-01T06:30:26.979132Z","iopub.status.idle":"2022-02-01T06:30:27.021188Z","shell.execute_reply.started":"2022-02-01T06:30:26.979101Z","shell.execute_reply":"2022-02-01T06:30:27.020339Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"##### **Dealing with missingness**\nHere, we deal with missing values in the dataset. We first identify missing columns then we dig deeper to understand the nature of missingness.\n\nWe would examine rows with missing values. And conclude one of two things.\n1. Values are missing at random\n2. There is a pattern to the missingness.\n\nThe nature of missingness will determine the approach to handling it. ","metadata":{}},{"cell_type":"code","source":"#display all columns that have missing values\nmissing_columns = train.columns.values[train.isnull().any()]\nmissing_columns\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.022562Z","iopub.execute_input":"2022-02-01T06:30:27.022870Z","iopub.status.idle":"2022-02-01T06:30:27.034554Z","shell.execute_reply.started":"2022-02-01T06:30:27.022828Z","shell.execute_reply":"2022-02-01T06:30:27.033885Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"missing_columns = validate.columns.values[validate.isnull().any()]\nmissing_columns","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.035850Z","iopub.execute_input":"2022-02-01T06:30:27.036670Z","iopub.status.idle":"2022-02-01T06:30:27.051633Z","shell.execute_reply.started":"2022-02-01T06:30:27.036626Z","shell.execute_reply":"2022-02-01T06:30:27.050479Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"##### **Notes**\nBoth the validation and training set have similar missing values so we would treat them the same.","metadata":{}},{"cell_type":"code","source":"#train[train.v68.isna()].iloc[20:50, :]\n#train[train.v24.isna()].iloc[20:50, :]\n#train[train.v99.isna()].iloc[10:50, 10:]\n#train[train.v95.isna()].iloc[:, 10:]\ntrain[train.v85.isna()].iloc[:, 10:]\n\n## The commented out code prints observations with missing columns","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.053320Z","iopub.execute_input":"2022-02-01T06:30:27.054359Z","iopub.status.idle":"2022-02-01T06:30:27.094744Z","shell.execute_reply.started":"2022-02-01T06:30:27.054299Z","shell.execute_reply":"2022-02-01T06:30:27.093714Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"##### **Notes**\n* The seems to be a pattern to the first two missing rows(**v1** and **v68**). We can use KNN impute for those columns. \n\n* Column **v95** seems to be missing too much information with 2/3 of value missing. I believe it should be dropped in it's entirity.\n\n\n","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns='v95')\nvalidate = validate.drop(columns='v95')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.098331Z","iopub.execute_input":"2022-02-01T06:30:27.098934Z","iopub.status.idle":"2022-02-01T06:30:27.106445Z","shell.execute_reply.started":"2022-02-01T06:30:27.098885Z","shell.execute_reply":"2022-02-01T06:30:27.105030Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Duplicate Values\nHere we treat duplicated rows","metadata":{}},{"cell_type":"code","source":"train = train.loc[~train.duplicated(), :]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.108129Z","iopub.execute_input":"2022-02-01T06:30:27.109073Z","iopub.status.idle":"2022-02-01T06:30:27.129846Z","shell.execute_reply.started":"2022-02-01T06:30:27.108968Z","shell.execute_reply":"2022-02-01T06:30:27.129104Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\n","metadata":{}},{"cell_type":"code","source":"train[numeric_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.131285Z","iopub.execute_input":"2022-02-01T06:30:27.134227Z","iopub.status.idle":"2022-02-01T06:30:27.170411Z","shell.execute_reply.started":"2022-02-01T06:30:27.134166Z","shell.execute_reply":"2022-02-01T06:30:27.169120Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"##### Notes\n\n* The the columns seem to be on very varying scales. We might need to scale the values while preprocessing.\n\n","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\ncorr = train[numeric_columns].corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 10, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5,)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.171707Z","iopub.execute_input":"2022-02-01T06:30:27.172234Z","iopub.status.idle":"2022-02-01T06:30:27.551561Z","shell.execute_reply.started":"2022-02-01T06:30:27.172189Z","shell.execute_reply":"2022-02-01T06:30:27.550700Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Above is a Correlation plot. The hue represents the level of interaction with a corresponding column. Here we see that only a few of the values are highly correlated (Inversely and otherwise). \n\n* That is we do not have too much interaction between columns. Such interactions can usually complicate modelling. \n\n* v42 and v7 seem to be highly correlated.\n","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=\"v42\", y=\"v7\", data=train)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.556230Z","iopub.execute_input":"2022-02-01T06:30:27.556504Z","iopub.status.idle":"2022-02-01T06:30:27.826564Z","shell.execute_reply.started":"2022-02-01T06:30:27.556470Z","shell.execute_reply":"2022-02-01T06:30:27.825853Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The plot about show that v42 and v7 have a perfectly linear realtionship. However, this can cause the model to needlessly overfit. \n\nAfter displaying the values below. We see that ***v42*** and ***v7*** are merely multiples of each other. We can remove one.","metadata":{}},{"cell_type":"code","source":"train[[\"v42\", 'v7']]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.827767Z","iopub.execute_input":"2022-02-01T06:30:27.828551Z","iopub.status.idle":"2022-02-01T06:30:27.845865Z","shell.execute_reply.started":"2022-02-01T06:30:27.828511Z","shell.execute_reply":"2022-02-01T06:30:27.844860Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train = train.drop(columns='v42')\nvalidate = validate.drop(columns='v42')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.847332Z","iopub.execute_input":"2022-02-01T06:30:27.848072Z","iopub.status.idle":"2022-02-01T06:30:27.861624Z","shell.execute_reply.started":"2022-02-01T06:30:27.848005Z","shell.execute_reply":"2022-02-01T06:30:27.860514Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=\"v8\", y=\"v5\", data=train)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:27.863113Z","iopub.execute_input":"2022-02-01T06:30:27.864110Z","iopub.status.idle":"2022-02-01T06:30:28.094902Z","shell.execute_reply.started":"2022-02-01T06:30:27.864069Z","shell.execute_reply":"2022-02-01T06:30:28.094136Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"v5\tv8\tv7\tv91\tv20\tv100\tv53\tv42","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=7, figsize=(20,4))\n\nsns.boxplot(x=\"classlabel\", y=\"v8\", data=train, ax = axes[0])\nsns.boxplot(x=\"classlabel\", y=\"v7\", data=train, ax = axes[1])\nsns.boxplot(x=\"classlabel\", y=\"v91\", data=train, ax=  axes[2])\nsns.boxplot(x=\"classlabel\", y=\"v20\", data=train,ax = axes[3])\nsns.boxplot(x=\"classlabel\", y=\"v100\", data=train, ax = axes[4])\nsns.boxplot(x=\"classlabel\", y=\"v53\", data=train, ax = axes[5])\nsns.boxplot(x=\"classlabel\", y=\"v5\", data=train, ax =axes[6])\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:28.096491Z","iopub.execute_input":"2022-02-01T06:30:28.096979Z","iopub.status.idle":"2022-02-01T06:30:28.960425Z","shell.execute_reply.started":"2022-02-01T06:30:28.096943Z","shell.execute_reply":"2022-02-01T06:30:28.959803Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Notes\nThe plots above, plot the distribution of the numerical values against our classlabels. \n\n* We observe that the differences between the distributions in each class varies from variable to variable. A big difference might mean that variable has some predicton power.\n\n* We also see outliers. There is no one way to determine if a value is an outlier. Nevertheless, the inter quartile range method illusatrated by the plots above is a widely used method. The dots represents values outside the data range.\n\n* It would pay to look closer at these dstributions.","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=6, figsize=(20,4))\n\nsns.histplot(x=train.v5, ax = axes[0])\nsns.histplot(x=train.v8, ax = axes[0])\nsns.histplot(x=train.v7, ax = axes[1])\nsns.histplot(x=train.v91, ax=  axes[2])\nsns.histplot(x=train.v20,ax = axes[3])\nsns.histplot(x=train.v100, ax = axes[4])\nsns.histplot(x=train.v53, ax = axes[5])\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:28.961809Z","iopub.execute_input":"2022-02-01T06:30:28.962186Z","iopub.status.idle":"2022-02-01T06:30:39.827694Z","shell.execute_reply.started":"2022-02-01T06:30:28.962151Z","shell.execute_reply":"2022-02-01T06:30:39.826770Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm\nsns.distplot(x=train.v91)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:39.829503Z","iopub.execute_input":"2022-02-01T06:30:39.830027Z","iopub.status.idle":"2022-02-01T06:30:40.166680Z","shell.execute_reply.started":"2022-02-01T06:30:39.829962Z","shell.execute_reply":"2022-02-01T06:30:40.165746Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"#### Notes\nMost of the data distributions are skeewed. Some models before better with more symetric distributions. We can perform some transformation on them.","metadata":{}},{"cell_type":"markdown","source":"## Explore Categoral Columns\n\nHere we plot  the distribution of the class labels across the catogories in each varaible. **Less homogenuity might indicate higher prediction power**.  As those columns are likely to differentiate the data more.\n\n\n","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=3, figsize=(20,4))\n\n#v1 column\nclass_by_v1 = train.groupby(['v1'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v1\", y=\"percentage\", hue=\"classlabel\", data=class_by_v1, ax = axes[0])\n\n#v2\nclass_by_v2 = train.groupby(['v2'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v2\", y=\"percentage\", hue=\"classlabel\", data=class_by_v2, ax = axes[1])\n\nclass_by_v33 = train.groupby(['v33'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v33\", y=\"percentage\", hue=\"classlabel\", data=class_by_v33, ax = axes[2])\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:40.168438Z","iopub.execute_input":"2022-02-01T06:30:40.168764Z","iopub.status.idle":"2022-02-01T06:30:40.827112Z","shell.execute_reply.started":"2022-02-01T06:30:40.168716Z","shell.execute_reply":"2022-02-01T06:30:40.826067Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=3, figsize=(20,4))\n\nclass_by_v68 = train.groupby(['v68'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v68\", y=\"percentage\", hue=\"classlabel\", data=class_by_v68, ax = axes[0])\n\nclass_by_v50 = train.groupby(['v50'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v50\", y=\"percentage\", hue=\"classlabel\", data=class_by_v50, ax = axes[1])\n\n#v1 column\nclass_by_v24 = train.groupby(['v24'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v24\", y=\"percentage\", hue=\"classlabel\", data=class_by_v24, ax = axes[2])\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:40.828578Z","iopub.execute_input":"2022-02-01T06:30:40.828968Z","iopub.status.idle":"2022-02-01T06:30:41.334844Z","shell.execute_reply.started":"2022-02-01T06:30:40.828848Z","shell.execute_reply":"2022-02-01T06:30:41.333782Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=3, figsize=(20,4))\n\n\n#v2\nclass_by_v9 = train.groupby(['v9'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v9\", y=\"percentage\", hue=\"classlabel\", data=class_by_v9, ax = axes[0])\n\nclass_by_v85 = train.groupby(['v85'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v85\", y=\"percentage\", hue=\"classlabel\", data=class_by_v85, ax = axes[1])\n\nclass_by_v28= train.groupby(['v28'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v28\", y=\"percentage\", hue=\"classlabel\", data=class_by_v28, ax = axes[2])\n\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:41.336637Z","iopub.execute_input":"2022-02-01T06:30:41.336985Z","iopub.status.idle":"2022-02-01T06:30:41.831318Z","shell.execute_reply.started":"2022-02-01T06:30:41.336936Z","shell.execute_reply":"2022-02-01T06:30:41.830320Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nclass_by_v44 = train.groupby(['v44'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v44\", y=\"percentage\", hue=\"classlabel\", data=class_by_v44, ax = axes[0])\n\nclass_by_v99 = train.groupby(['v99'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v99\", y=\"percentage\", hue=\"classlabel\", data=class_by_v99, ax = axes[1])\n\n#v44 column\nclass_by_v44 = train.groupby(['v44'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v44\", y=\"percentage\", hue=\"classlabel\", data=class_by_v44, ax = axes[2])\n\n#v84\nclass_by_v84 = train.groupby(['v84'])[\"classlabel\"].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"v84\", y=\"percentage\", hue=\"classlabel\", data=class_by_v84, ax = axes[3])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:41.832819Z","iopub.execute_input":"2022-02-01T06:30:41.833614Z","iopub.status.idle":"2022-02-01T06:30:42.744335Z","shell.execute_reply.started":"2022-02-01T06:30:41.833543Z","shell.execute_reply":"2022-02-01T06:30:42.743180Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"##### **Notes**\n\nfeature v9 and v28. Are unlikely to differentiate the classes. As the classes have a similar variation in both groups.\n","metadata":{}},{"cell_type":"markdown","source":"\n\n\n### Preprocessing and Modelling\n\nHere we finally apply a model to the cleaned data from our previous steps. However, it is very likely that would need to perform additional preprocessing steps on the data. \n1. Scaling\n2. feature engineering\n3. ***resampling***.\n\nDue to the imbalanced nature of the dataset the model might overfit on training examples from the dominant class. To counter this we can perform a number of resampling operations. We wll use the results of cross validation to judge model performance.\n\n\n**Feature Selection**: Since we know so little about the data a manual feature selection would not be well informed. However we can employ techniques like regularization, varible importance metrics or maybe even a dimensional .\n","metadata":{}},{"cell_type":"markdown","source":"#### Preprocessing\nWe would perform a number of operations before we finally tackle our model.\nWe will follow these steps:\n\n1. Encode categorical variables\n2. Impute or drop missing values\n3. Scale numerical features.\n4. Downsample or upsample. \n5. Try SMOTE\n6. Create two baseline models. One  linear and the other treebased.","metadata":{}},{"cell_type":"markdown","source":"1, Imputing Missing values.\n\nWe determined earlier that the our data doesn't seem to be ***missing at random*** especially in the first two columns. As such, we can estimate the missng values based on observered patterns. However, to use such imputing techniques we have to encode our catogories as numbers.","metadata":{}},{"cell_type":"code","source":"#recoding non null labels as numbers\ntrain_proc = train.apply(lambda series: pd.Series(LabelEncoder().fit_transform(series[series.notnull()]),index=series[series.notnull()].index))\ntrain_proc ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:42.746102Z","iopub.execute_input":"2022-02-01T06:30:42.746453Z","iopub.status.idle":"2022-02-01T06:30:42.828047Z","shell.execute_reply.started":"2022-02-01T06:30:42.746408Z","shell.execute_reply":"2022-02-01T06:30:42.826923Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#recoding non null labels as numbers\nvalidate_proc = validate.apply(lambda series: pd.Series(LabelEncoder().fit_transform(series[series.notnull()]),index=series[series.notnull()].index))\nvalidate_proc ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:42.829332Z","iopub.execute_input":"2022-02-01T06:30:42.829555Z","iopub.status.idle":"2022-02-01T06:30:42.888503Z","shell.execute_reply.started":"2022-02-01T06:30:42.829527Z","shell.execute_reply":"2022-02-01T06:30:42.887683Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#Using the KNN method for the first two columns\nimputer = KNNImputer(n_neighbors=4)\ncolumns = train_proc.columns.values\ntrain_proc[columns] = imputer.fit_transform(train_proc)\n\n##impute for validate\n#Using the KNN method for the first two columns\nimputer = KNNImputer(n_neighbors=4)\ncolumns = validate.columns.values\nvalidate_proc[columns] = imputer.fit_transform(validate_proc)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:42.889922Z","iopub.execute_input":"2022-02-01T06:30:42.890185Z","iopub.status.idle":"2022-02-01T06:30:43.062193Z","shell.execute_reply.started":"2022-02-01T06:30:42.890148Z","shell.execute_reply":"2022-02-01T06:30:43.061274Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#display all columns that have missing values\nmissing_columns = train_proc.columns.values[train_proc.isnull().any()]\nmissing_columns\n\nmissing_columns_val = validate_proc.columns.values[validate_proc.isnull().any()]\nmissing_columns_val","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.063520Z","iopub.execute_input":"2022-02-01T06:30:43.063762Z","iopub.status.idle":"2022-02-01T06:30:43.079945Z","shell.execute_reply.started":"2022-02-01T06:30:43.063730Z","shell.execute_reply":"2022-02-01T06:30:43.078968Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#Convert Category Columns back to categories.\ncat_columns = train_proc.columns.values[train.dtypes == 'category']\ntrain_proc[cat_columns] = train_proc[cat_columns].astype(\"int\").astype(\"category\")\ntrain_proc[\"classlabel\"] = train_proc[\"classlabel\"].astype(\"int\").astype(\"category\")\n\ncat_columns = validate_proc.columns.values[train.dtypes == 'category']\nvalidate_proc[cat_columns] = validate_proc[cat_columns].astype(\"int\").astype(\"category\")\nvalidate_proc[\"classlabel\"] = validate_proc[\"classlabel\"].astype(\"int\").astype(\"category\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.081411Z","iopub.execute_input":"2022-02-01T06:30:43.081662Z","iopub.status.idle":"2022-02-01T06:30:43.118069Z","shell.execute_reply.started":"2022-02-01T06:30:43.081631Z","shell.execute_reply":"2022-02-01T06:30:43.117244Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"#### Feature  Engineering \n* Here we would rescale our numerical features. We shouldn't do this before spliting the dataset as that will cause some information leakage.\n\n* Transform them into polynomials. This can help our model capture more complex relationships but can lead to overfitting. \n\n* Our label encoding is a well used technique. But it can add some error to our model in the sense that the arbitrary number ordering can be misinterpreted by some models. However, we can recode our using another well used technique 'one hot encoding'. \n\nWe should introduce the pipeline module that makes the transformations more manageble and reuseable on a validation set.\n\n","metadata":{}},{"cell_type":"code","source":"#get indexes for numerical columns\nnum_columns = train_proc.columns.values[train_proc.dtypes == 'float64']\nnum_columns","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.120288Z","iopub.execute_input":"2022-02-01T06:30:43.120703Z","iopub.status.idle":"2022-02-01T06:30:43.128225Z","shell.execute_reply.started":"2022-02-01T06:30:43.120668Z","shell.execute_reply":"2022-02-01T06:30:43.126923Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#Scaling ensures that all the columns are on on the same scale/unit\nscaler = StandardScaler()\nvalidate_proc[num_columns] = scaler.fit_transform(validate_proc[num_columns])\n\n#Scaling ensures that all the columns are on on the same scale/unit\nscaler = StandardScaler()\ntrain_proc[num_columns] = scaler.fit_transform(train_proc[num_columns])","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:51:51.133955Z","iopub.execute_input":"2022-02-01T06:51:51.134969Z","iopub.status.idle":"2022-02-01T06:51:51.154396Z","shell.execute_reply.started":"2022-02-01T06:51:51.134912Z","shell.execute_reply":"2022-02-01T06:51:51.153357Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\npoly_fit = poly.fit_transform(train_proc[num_columns])\n\npoly_frame = pd.DataFrame(poly_fit, columns=[f\"poly1_{i}\" for i in range(poly_fit.shape[1])])\n\n#reset columns indexes for smooth join\ntrain_proc = train_proc.reset_index().drop(columns=\"index\")\n\n#join\npd.concat([train_proc, poly_frame], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:31:51.502881Z","iopub.execute_input":"2022-02-01T06:31:51.503210Z","iopub.status.idle":"2022-02-01T06:31:51.515186Z","shell.execute_reply.started":"2022-02-01T06:31:51.503177Z","shell.execute_reply":"2022-02-01T06:31:51.514077Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:45:39.866219Z","iopub.execute_input":"2022-02-01T06:45:39.866760Z","iopub.status.idle":"2022-02-01T06:45:39.912333Z","shell.execute_reply.started":"2022-02-01T06:45:39.866722Z","shell.execute_reply":"2022-02-01T06:45:39.911533Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"#pd.merge(train_proc, poly_frame, how='inner', on=['v5:poly1_3'])","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.164990Z","iopub.execute_input":"2022-02-01T06:30:43.165315Z","iopub.status.idle":"2022-02-01T06:30:43.178877Z","shell.execute_reply.started":"2022-02-01T06:30:43.165276Z","shell.execute_reply":"2022-02-01T06:30:43.177585Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_proc = train_proc.loc[~train_proc.duplicated(), :]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.180440Z","iopub.execute_input":"2022-02-01T06:30:43.180806Z","iopub.status.idle":"2022-02-01T06:30:43.342576Z","shell.execute_reply.started":"2022-02-01T06:30:43.180776Z","shell.execute_reply":"2022-02-01T06:30:43.341439Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"#### **Baseline Model**\n\n","metadata":{}},{"cell_type":"code","source":"# Extract Features and Target Columns\nfeatures = train_proc.columns.values[train_proc.columns.values != \"classlabel\"]\nX_train = train_proc[features]\ny_train = train_proc[\"classlabel\"]\n\n# Extract Features and Target Columns\nfeatures = validate_proc.columns.values[validate_proc.columns.values != \"classlabel\"]\nX_test = validate_proc[features]\ny_test = validate_proc[\"classlabel\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.343940Z","iopub.status.idle":"2022-02-01T06:30:43.344327Z","shell.execute_reply.started":"2022-02-01T06:30:43.344142Z","shell.execute_reply":"2022-02-01T06:30:43.344165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.345726Z","iopub.status.idle":"2022-02-01T06:30:43.346353Z","shell.execute_reply.started":"2022-02-01T06:30:43.346176Z","shell.execute_reply":"2022-02-01T06:30:43.346195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.347127Z","iopub.status.idle":"2022-02-01T06:30:43.347850Z","shell.execute_reply.started":"2022-02-01T06:30:43.347646Z","shell.execute_reply":"2022-02-01T06:30:43.347669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Class Imbalance\n\nThe distribution of the classes is imbalanced, as illustrated below. To deal with this we can employ a number of techniques. Such downsampling or upsampling.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"classlabel\", data=train_proc)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.349223Z","iopub.status.idle":"2022-02-01T06:30:43.349559Z","shell.execute_reply.started":"2022-02-01T06:30:43.349376Z","shell.execute_reply":"2022-02-01T06:30:43.349399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define pipeline\nsteps = [('under', RandomUnderSampler(0.1)), ('model', LGBMClassifier())]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Create the parameter grid\ngbm_param_grid = {'model__learning_rate': np.arange(.05, 1, .05), 'model__max_depth': np.arange(3,20, 2),'model__n_estimators': np.arange(20, 100, 25), }\n\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Perform RandomizedSearchCV\nrandomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid, n_iter=2, scoring='roc_auc', cv=cv, verbose=1)\n\n# Fit the estimator\nrandomized_roc_auc.fit(X_train, y_train)\n\n# Compute metrics\nprint(randomized_roc_auc.best_score_)\nprint(randomized_roc_auc.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.350524Z","iopub.status.idle":"2022-02-01T06:30:43.350838Z","shell.execute_reply.started":"2022-02-01T06:30:43.350668Z","shell.execute_reply":"2022-02-01T06:30:43.350691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Final Model\n\nWe use the final estimator to train a final model. We would then use this model to perform inference on the validation set.","metadata":{}},{"cell_type":"code","source":"y_preds = randomized_roc_auc.predict(X_test)\n\nf1_score(y_preds, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.352362Z","iopub.status.idle":"2022-02-01T06:30:43.352678Z","shell.execute_reply.started":"2022-02-01T06:30:43.352510Z","shell.execute_reply":"2022-02-01T06:30:43.352531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Notes\nThe dataset seems to be overfit on the training set and performs woefully on the training set.  ","metadata":{}},{"cell_type":"markdown","source":"#### Model Valuation\n\nDue to the imbalanced nature of the dataset. The accuracy score can be misleading, to upset this we can use the F1 score as our key evaluation metric. As it considers the nature of classification errors.\n\nThe dataset seems to be overfit on the training set and performs woefully on the training set. We can:\n1. introduce regularization\n2. reduce variance by decreasing the model's complexity. \n3. Introduce feature selection\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-01T06:30:43.354177Z","iopub.status.idle":"2022-02-01T06:30:43.354823Z","shell.execute_reply.started":"2022-02-01T06:30:43.354619Z","shell.execute_reply":"2022-02-01T06:30:43.354647Z"},"trusted":true},"execution_count":null,"outputs":[]}]}